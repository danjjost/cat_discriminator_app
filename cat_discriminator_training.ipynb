{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# üêà Cat Discriminator"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üìù Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.optim as optim\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "import albumentations as A\n",
    "\n",
    "import torchvision.transforms as transforms\n",
    "\n",
    "import os\n",
    "\n",
    "from src.cat_discriminator_neural_net import CatDiscriminatorNeuralNet\n",
    "\n",
    "from src.augmentation.data_augmenter import DataAugmenter\n",
    "from src.cats_dataset import CatsDataset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üîß Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "image_size = 512\n",
    "\n",
    "learning_rate = 0.001\n",
    "batches_between_saves = 1\n",
    "number_of_batches = 100\n",
    "\n",
    "batch_size = 32\n",
    "\n",
    "num_cores = 4\n",
    "\n",
    "saved_model_path = \"trained_networks/experiment-binary-classifier-with-synthetic-data.pth\"\n",
    "\n",
    "root_directory = 'data/'\n",
    "control_folders=['training/bathroom-cat', 'synthetic/tortoiseshell', 'synthetic/control', 'training/control']\n",
    "captain_present_folders=['training/captain', 'synthetic/tabby']"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üåê Create Transforms"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "transforms = transforms.Compose([\n",
    "    DataAugmenter(image_size, augment_images=True),\n",
    "    transforms.ToTensor(),\n",
    "])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üö¶ Load Training Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "dataset = CatsDataset(\n",
    "    root_dir=root_directory,\n",
    "    control_folders=control_folders,\n",
    "    captain_present_folders=captain_present_folders,\n",
    "    transform=transforms\n",
    ")\n",
    "\n",
    "dataloader = DataLoader(dataset, batch_size=batch_size, shuffle=True, num_workers=num_cores)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### ü•æ Initialize the Neural Net"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "net = CatDiscriminatorNeuralNet(learning_rate, saved_model_path)\n",
    "net.cuda();"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### üèÉ‚Äç‚ôÇÔ∏è‚Äç‚û°Ô∏è Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training batch:  0\n",
      "Loss: 0.0877\n",
      "Saving model...\n",
      "Training batch:  1\n",
      "Loss: 0.0955\n",
      "Saving model...\n",
      "Training batch:  2\n",
      "Loss: 0.1016\n",
      "Saving model...\n",
      "Training batch:  3\n",
      "Loss: 0.0859\n",
      "Saving model...\n",
      "Training batch:  4\n",
      "Loss: 0.0986\n",
      "Saving model...\n",
      "Training batch:  5\n",
      "Loss: 0.0931\n",
      "Saving model...\n",
      "Training batch:  6\n",
      "Loss: 0.0915\n",
      "Saving model...\n",
      "Training batch:  7\n",
      "Loss: 0.0882\n",
      "Saving model...\n",
      "Training batch:  8\n",
      "Loss: 0.0827\n",
      "Saving model...\n",
      "Training batch:  9\n",
      "Loss: 0.0865\n",
      "Saving model...\n",
      "Training batch:  10\n",
      "Loss: 0.0949\n",
      "Saving model...\n",
      "Training batch:  11\n",
      "Loss: 0.0931\n",
      "Saving model...\n",
      "Training batch:  12\n",
      "Loss: 0.1038\n",
      "Saving model...\n",
      "Training batch:  13\n",
      "Loss: 0.1083\n",
      "Saving model...\n",
      "Training batch:  14\n",
      "Loss: 0.0817\n",
      "Saving model...\n",
      "Training batch:  15\n",
      "Loss: 0.0886\n",
      "Saving model...\n",
      "Training batch:  16\n",
      "Loss: 0.0797\n",
      "Saving model...\n",
      "Training batch:  17\n",
      "Loss: 0.0754\n",
      "Saving model...\n",
      "Training batch:  18\n",
      "Loss: 0.0800\n",
      "Saving model...\n",
      "Training batch:  19\n",
      "Loss: 0.0921\n",
      "Saving model...\n",
      "Training batch:  20\n",
      "Loss: 0.0792\n",
      "Saving model...\n",
      "Training batch:  21\n",
      "Loss: 0.0904\n",
      "Saving model...\n",
      "Training batch:  22\n",
      "Loss: 0.0927\n",
      "Saving model...\n",
      "Training batch:  23\n",
      "Loss: 0.0928\n",
      "Saving model...\n",
      "Training batch:  24\n",
      "Loss: 0.0848\n",
      "Saving model...\n",
      "Training batch:  25\n",
      "Loss: 0.0845\n",
      "Saving model...\n",
      "Training batch:  26\n",
      "Loss: 0.0756\n",
      "Saving model...\n",
      "Training batch:  27\n",
      "Loss: 0.0980\n",
      "Saving model...\n",
      "Training batch:  28\n",
      "Loss: 0.0805\n",
      "Saving model...\n",
      "Training batch:  29\n",
      "Loss: 0.0673\n",
      "Saving model...\n",
      "Training batch:  30\n",
      "Loss: 0.0793\n",
      "Saving model...\n",
      "Training batch:  31\n",
      "Loss: 0.0726\n",
      "Saving model...\n",
      "Training batch:  32\n",
      "Loss: 0.0778\n",
      "Saving model...\n",
      "Training batch:  33\n",
      "Loss: 0.0769\n",
      "Saving model...\n",
      "Training batch:  34\n",
      "Loss: 0.0740\n",
      "Saving model...\n",
      "Training batch:  35\n",
      "Loss: 0.0678\n",
      "Saving model...\n",
      "Training batch:  36\n",
      "Loss: 0.0972\n",
      "Saving model...\n",
      "Training batch:  37\n",
      "Loss: 0.0768\n",
      "Saving model...\n",
      "Training batch:  38\n",
      "Loss: 0.0834\n",
      "Saving model...\n",
      "Training batch:  39\n",
      "Loss: 0.0749\n",
      "Saving model...\n",
      "Training batch:  40\n",
      "Loss: 0.0806\n",
      "Saving model...\n",
      "Training batch:  41\n",
      "Loss: 0.0650\n",
      "Saving model...\n",
      "Training batch:  42\n",
      "Loss: 0.0772\n",
      "Saving model...\n",
      "Training batch:  43\n",
      "Loss: 0.0782\n",
      "Saving model...\n",
      "Training batch:  44\n",
      "Loss: 0.0815\n",
      "Saving model...\n",
      "Training batch:  45\n",
      "Loss: 0.0807\n",
      "Saving model...\n",
      "Training batch:  46\n",
      "Loss: 0.0807\n",
      "Saving model...\n",
      "Training batch:  47\n",
      "Loss: 0.0682\n",
      "Saving model...\n",
      "Training batch:  48\n",
      "Loss: 0.0694\n",
      "Saving model...\n",
      "Training batch:  49\n",
      "Loss: 0.0791\n",
      "Saving model...\n",
      "Training batch:  50\n",
      "Loss: 0.0706\n",
      "Saving model...\n",
      "Training batch:  51\n",
      "Loss: 0.0919\n",
      "Saving model...\n",
      "Training batch:  52\n",
      "Loss: 0.0717\n",
      "Saving model...\n",
      "Training batch:  53\n",
      "Loss: 0.0795\n",
      "Saving model...\n",
      "Training batch:  54\n",
      "Loss: 0.0712\n",
      "Saving model...\n",
      "Training batch:  55\n",
      "Loss: 0.0901\n",
      "Saving model...\n",
      "Training batch:  56\n",
      "Loss: 0.0720\n",
      "Saving model...\n",
      "Training batch:  57\n",
      "Loss: 0.0822\n",
      "Saving model...\n",
      "Training batch:  58\n",
      "Loss: 0.0667\n",
      "Saving model...\n",
      "Training batch:  59\n",
      "Loss: 0.0748\n",
      "Saving model...\n",
      "Training batch:  60\n",
      "Loss: 0.0652\n",
      "Saving model...\n",
      "Training batch:  61\n",
      "Loss: 0.0687\n",
      "Saving model...\n",
      "Training batch:  62\n",
      "Loss: 0.0665\n",
      "Saving model...\n",
      "Training batch:  63\n",
      "Loss: 0.0879\n",
      "Saving model...\n",
      "Training batch:  64\n",
      "Loss: 0.0913\n",
      "Saving model...\n",
      "Training batch:  65\n",
      "Loss: 0.0654\n",
      "Saving model...\n",
      "Training batch:  66\n",
      "Loss: 0.0739\n",
      "Saving model...\n",
      "Training batch:  67\n",
      "Loss: 0.0733\n",
      "Saving model...\n",
      "Training batch:  68\n",
      "Loss: 0.0900\n",
      "Saving model...\n",
      "Training batch:  69\n",
      "Loss: 0.0740\n",
      "Saving model...\n",
      "Training batch:  70\n",
      "Loss: 0.0629\n",
      "Saving model...\n",
      "Training batch:  71\n",
      "Loss: 0.0637\n",
      "Saving model...\n",
      "Training batch:  72\n",
      "Loss: 0.0769\n",
      "Saving model...\n",
      "Training batch:  73\n",
      "Loss: 0.0784\n",
      "Saving model...\n",
      "Training batch:  74\n",
      "Loss: 0.0691\n",
      "Saving model...\n",
      "Training batch:  75\n",
      "Loss: 0.0691\n",
      "Saving model...\n",
      "Training batch:  76\n",
      "Loss: 0.0884\n",
      "Saving model...\n",
      "Training batch:  77\n",
      "Loss: 0.0770\n",
      "Saving model...\n",
      "Training batch:  78\n",
      "Loss: 0.0657\n",
      "Saving model...\n",
      "Training batch:  79\n",
      "Loss: 0.0624\n",
      "Saving model...\n",
      "Training batch:  80\n",
      "Loss: 0.0613\n",
      "Saving model...\n",
      "Training batch:  81\n",
      "Loss: 0.0636\n",
      "Saving model...\n",
      "Training batch:  82\n",
      "Loss: 0.0717\n",
      "Saving model...\n",
      "Training batch:  83\n",
      "Loss: 0.0663\n",
      "Saving model...\n",
      "Training batch:  84\n",
      "Loss: 0.0732\n",
      "Saving model...\n",
      "Training batch:  85\n",
      "Loss: 0.0598\n",
      "Saving model...\n",
      "Training batch:  86\n",
      "Loss: 0.0672\n",
      "Saving model...\n",
      "Training batch:  87\n",
      "Loss: 0.0695\n",
      "Saving model...\n",
      "Training batch:  88\n",
      "Loss: 0.0784\n",
      "Saving model...\n",
      "Training batch:  89\n",
      "Loss: 0.0667\n",
      "Saving model...\n",
      "Training batch:  90\n",
      "Loss: 0.0701\n",
      "Saving model...\n",
      "Training batch:  91\n",
      "Loss: 0.0741\n",
      "Saving model...\n",
      "Training batch:  92\n",
      "Loss: 0.0682\n",
      "Saving model...\n",
      "Training batch:  93\n",
      "Loss: 0.0639\n",
      "Saving model...\n",
      "Training batch:  94\n",
      "Loss: 0.0686\n",
      "Saving model...\n",
      "Training batch:  95\n",
      "Loss: 0.0646\n",
      "Saving model...\n",
      "Training batch:  96\n",
      "Loss: 0.0658\n",
      "Saving model...\n",
      "Training batch:  97\n",
      "Loss: 0.0595\n",
      "Saving model...\n",
      "Training batch:  98\n",
      "Loss: 0.0637\n",
      "Saving model...\n",
      "Training batch:  99\n",
      "Loss: 0.0681\n",
      "Saving model...\n"
     ]
    }
   ],
   "source": [
    "batch_number = 0\n",
    "\n",
    "while batch_number < number_of_batches:\n",
    "    print('Training batch: ', batch_number)\n",
    "    net.run_training_batch(learning_rate, dataloader)\n",
    "\n",
    "    if batch_number % batches_between_saves == 0:\n",
    "        print('Saving model...')\n",
    "        torch.save(net.state_dict(), saved_model_path)\n",
    "\n",
    "    batch_number += 1\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
